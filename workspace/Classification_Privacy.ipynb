{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwDK47gfLsYf"
   },
   "source": [
    "# Implement Differential Privacy with TensorFlow Privacy\n",
    "\n",
    "Modified from [official tutorial](https://github.com/tensorflow/privacy/blob/master/tutorials/Classification_Privacy.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ef56gCUqrdVn"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 1.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_fVhfUyeI3d"
   },
   "source": [
    "Install TensorFlow Privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RseeuA7veIHU"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU1p8N7M5Mmn"
   },
   "source": [
    "## Load and pre-process the dataset\n",
    "\n",
    "Load the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1ML23FlueTr"
   },
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "train_data, train_labels = train\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "assert train_data.min() == 0.\n",
    "assert train_data.max() == 1.\n",
    "assert test_data.min() == 0.\n",
    "assert test_data.max() == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVDcswOCtlr3"
   },
   "source": [
    "## Define and tune learning model hyperparameters\n",
    "Set learning model hyperparamter values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXNp_25y7JP2"
   },
   "source": [
    "DP-SGD has three privacy-specific hyperparameters and one existing hyperamater that you must tune:\n",
    "\n",
    "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points. \n",
    "2. `noise_multiplier` (float) - The amount of noise sampled and added to gradients during training. Generally, more noise results in better privacy (often, but not necessarily, at the expense of lower utility).\n",
    "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. The number of microbatches should evenly divide the batch size. \n",
    "4. `learning_rate` (float) - This hyperparameter already exists in vanilla SGD. The higher the learning rate, the more each update matters. If the updates are noisy (such as when the additive noise is large compared to the clipping threshold), a low learning rate may help the training procedure converge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL7_lX5sHCTI"
   },
   "source": [
    "## Measure the differential privacy guarantee\n",
    "\n",
    "Perform a privacy analysis to measure the DP guarantee achieved by a training algorithm. Knowing the level of DP achieved enables the objective comparison of two training runs to determine which of the two is more privacy-preserving. At a high level, the privacy analysis measures how much a potential adversary can improve their guess about properties of any individual training point by observing the outcome of our training procedure (e.g., model updates and parameters). \n",
    "\n",
    "This guarantee is sometimes referred to as the **privacy budget**. A lower privacy budget bounds more tightly an adversary's ability to improve their guess. This ensures a stronger privacy guarantee. Intuitively, this is because it is harder for a single training point to affect the outcome of learning: for instance, the information contained in the training point cannot be memorized by the ML algorithm and the privacy of the individual who contributed this training point to the dataset is preserved.\n",
    "\n",
    "In this tutorial, the privacy analysis is performed in the framework of RÃ©nyi Differential Privacy (RDP), which is a relaxation of pure DP based on [this paper](https://arxiv.org/abs/1702.07476) that is particularly well suited for DP-SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUEk25pgmnm-"
   },
   "source": [
    "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
    "\n",
    "1.   Delta ($\\delta$) - Bounds the probability of the privacy guarantee not holding. A rule of thumb is to set it to be less than the inverse of the size of the training dataset. In this tutorial, it is set to **10^-5** as the MNIST dataset has 60,000 training points.\n",
    "2.   Epsilon ($\\epsilon$) - This is the privacy budget. It measures the strength of the privacy guarantee by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. A smaller value for $\\epsilon$ implies a better privacy guarantee. However, the $\\epsilon$ value is only an upper bound and a large value could still mean good privacy in practice.\n",
    "\n",
    "Tensorflow Privacy provides a tool, `compute_dp_sgd_privacy.py`, to compute the value of $\\epsilon$ given a fixed value of $\\delta$ and the following hyperparameters from the training process:\n",
    "\n",
    "1.   The total number of points in the training data, `n`.\n",
    "2. The `batch_size`.\n",
    "3.   The `noise_multiplier`.\n",
    "4. The number of `epochs` of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws8-nVuVDgtJ"
   },
   "outputs": [],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=250, noise_multiplier=1.3, epochs=15, delta=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-KyttEWFRDc"
   },
   "source": [
    "The tool reports that for the hyperparameters chosen above, the trained model has an $\\epsilon$ value of 1.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SA_9HMGBWFM3"
   },
   "source": [
    "## Summary\n",
    "In this tutorial, you learned about differential privacy (DP) and how you can implement DP principles in existing ML algorithms to provide privacy guarantees for training data. In particular, you learned how to:\n",
    "*   Wrap existing optimizers (e.g., SGD, Adam) into their differentially private counterparts using TensorFlow Privacy\n",
    "*   Tune hyperparameters introduced by differentially private machine learning\n",
    "*   Measure the privacy guarantee provided using analysis tools included in TensorFlow Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 250\n",
    "\n",
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 4.5]\n",
    "num_microbatches = 250\n",
    "learning_rate = 0.25\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "    raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "    \n",
    "acc = []\n",
    "val_acc = []\n",
    "eps = []\n",
    "alpha = []\n",
    "delta = []\n",
    "\n",
    "for i in range(len(noise_multiplier)):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, 8,\n",
    "                               strides=2,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPool2D(2, 1),\n",
    "        tf.keras.layers.Conv2D(32, 4,\n",
    "                               strides=2,\n",
    "                               padding='valid',\n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(2, 1),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = DPGradientDescentGaussianOptimizer(\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier[i],\n",
    "        num_microbatches=num_microbatches,\n",
    "        learning_rate=learning_rate)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          validation_data=(test_data, test_labels),\n",
    "          batch_size=batch_size)\n",
    "    \n",
    "    delta.append(round(time.time() - start, 2))\n",
    "    \n",
    "    acc.append(model.history.history['acc'][-1])\n",
    "    val_acc.append(model.history.history['val_acc'][-1])\n",
    "    (eps_, alpha_) = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=model.history.params['samples'],\n",
    "                                                                 batch_size=model.history.params['batch_size'],\n",
    "                                                                 noise_multiplier=noise_multiplier[i],\n",
    "                                                                 epochs=model.history.params['epochs'],\n",
    "                                                                 delta=1e-5)\n",
    "    eps.append(eps_)\n",
    "    alpha.append(alpha_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argsort(noise_multiplier)\n",
    "np.save(\"epsilon\", np.array(eps)[index])\n",
    "np.save(\"accuracy\", np.array(acc)[index])\n",
    "np.save(\"validation_accuracy\", np.array(val_acc)[index])\n",
    "np.save(\"elapsed_time\", np.array(delta)[index])\n",
    "np.save(\"noise_multiplier\", np.array(noise_multiplier)[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_multiplier = np.load(\"noise_multiplier.npy\")\n",
    "epsilon = np.load(\"epsilon.npy\")\n",
    "accuracy = np.load(\"accuracy.npy\")\n",
    "val_accuracy = np.load(\"validation_accuracy.npy\")\n",
    "time = np.load(\"elapsed_time.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_top, ax_center, ax_bottom) = plt.subplots(3, 1, sharex=True)\n",
    "\n",
    "fig.set_size_inches(12,12)\n",
    "fig.suptitle(\"Metrics evolution with noise\", fontsize=25, y=0.92)\n",
    "\n",
    "ax_top.scatter(noise_multiplier, epsilon, s=100, c=\"red\", alpha=0.3, edgecolors='black', marker=\"X\")\n",
    "ax_top.set_ylabel(\"epsilon\", fontsize=20)\n",
    "ax_top.grid(True, axis=\"y\", color='grey', linestyle='--', linewidth=.2)\n",
    "ax_top.grid(True, axis=\"x\", color='grey', linestyle=':', linewidth=.5)\n",
    "ax_top.xaxis.set_ticks(noise_multiplier)\n",
    "\n",
    "ax_center.scatter(noise_multiplier, val_accuracy, s=100, c=\"green\", alpha=0.3, edgecolors='black', marker=\"X\")\n",
    "ax_center.set_ylabel(\"accuracy\", fontsize=20)\n",
    "ax_center.grid(True, axis=\"y\", color='grey', linestyle='--', linewidth=.2)\n",
    "ax_center.grid(True, axis=\"x\", color='grey', linestyle=':', linewidth=.5)\n",
    "\n",
    "ax_bottom.scatter(noise_multiplier, time, s=100, c=\"blue\", alpha=0.3, edgecolors='black', marker=\"X\")\n",
    "ax_bottom.set_xlabel(\"noise multiplier\", fontsize=15)\n",
    "ax_bottom.set_ylabel(\"time [s]\", fontsize=20)\n",
    "ax_bottom.grid(True, axis=\"y\", color='grey', linestyle='--', linewidth=.2)\n",
    "ax_bottom.grid(True, axis=\"x\", color='grey', linestyle=':', linewidth=.5)\n",
    "ax_bottom.xaxis.set_ticklabels(noise_multiplier, rotation=45, fontsize=13)\n",
    "\n",
    "# plt.show()\n",
    "fig.savefig(\"perf_vs_privacy_new_title.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same model, without DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 250\n",
    "\n",
    "learning_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 8,\n",
    "                           strides=2,\n",
    "                           padding='same',\n",
    "                           activation='relu',\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Conv2D(32, 4,\n",
    "                           strides=2,\n",
    "                           padding='valid',\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2, 1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_data, test_labels),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "training_time = round(time.time() - start, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification_Privacy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
