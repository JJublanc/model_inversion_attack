{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy - attack model - part1 : Wasserstein GAN\n",
    "\n",
    "__Main goal__ : the main goal of this part is to use public data to build a generator able to produce the kind of output we want to reveal. Here we want to reveal images of hand written digits from 0 to 4. We use images of hand written digits from 5 to 9 to train the generator, so that the public and private dataset do not overlap.\n",
    "\n",
    "__Reference for the WGAN__ : \n",
    "* https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/\n",
    "* https://github.com/ilguyi/gans.tensorflow.v2/blob/master/tf.v2/01.dcgan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "###########\n",
    "## plots ##\n",
    "###########\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "######\n",
    "# TF #\n",
    "######\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, InputLayer, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "tf.__version__\n",
    "\n",
    "#########\n",
    "# utils #\n",
    "#########\n",
    "\n",
    "os.sys.path.append(\"./src\")\n",
    "from utils import plot_img\n",
    "from utils import load_mnist_data\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"gan\" in os.listdir(\"./model\"):\n",
    "    os.mkdir(\"model/gan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(TRAIN_BUF, BATCH_SIZE, TEST_BUF, data_type=\"public\"):\n",
    "    \n",
    "    train_images, train_labels, test_images, test_labels = load_mnist_data(data_type)\n",
    "\n",
    "    train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(train_images)\n",
    "        .shuffle(TRAIN_BUF)\n",
    "        .batch(BATCH_SIZE))\n",
    "\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(test_images)\n",
    "        .shuffle(TEST_BUF)\n",
    "        .batch(BATCH_SIZE))\n",
    "\n",
    "    plot_img(train_images, train_labels)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the generator and discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(latent_dim=64):\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    generator = Sequential()\n",
    "    generator.add(InputLayer(input_shape=(1, 1, latent_dim)))\n",
    "    generator.add(Dense(units=7 * 7 * 64, activation=\"relu\"))\n",
    "    generator.add(Reshape(target_shape=(7, 7, 64)))\n",
    "    generator.add(Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2),\n",
    "                                  kernel_initializer=init, padding=\"SAME\", activation=\"relu\"))\n",
    "    generator.add(Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2),\n",
    "                                  kernel_initializer=init, padding=\"SAME\", activation=\"relu\"))\n",
    "    generator.add(Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1),\n",
    "                                  kernel_initializer=init, padding=\"SAME\", activation=\"sigmoid\"))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    \n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(InputLayer(input_shape=DIMS))\n",
    "    discriminator.add(Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"))\n",
    "    discriminator.add(Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "number_of_disc_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the loss and the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(train_x):\n",
    "    \n",
    "    # Random object in the latent space\n",
    "    x = tf.random.normal([train_x.shape[0], 1, 1, 64])\n",
    "    \n",
    "    # Create real and fake outputs\n",
    "    real_output = discriminator(train_x)\n",
    "    fake_output = discriminator(generator(x))\n",
    "    \n",
    "    # Compute loss\n",
    "    disc_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "    gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "    return disc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One step of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(train_x, generator_optimizer, discriminator_optimizer, n_steps=4):\n",
    "    x = tf.random.normal([train_x.shape[0], 1, 1, 64])\n",
    "    for i in range(n_steps):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            \n",
    "            # Create true and fake outputs\n",
    "            real_output = discriminator(train_x)\n",
    "            fake_output = discriminator(generator(x))\n",
    "            \n",
    "            # Compute the loss\n",
    "            disc_loss = -tf.reduce_mean(real_output) + tf.reduce_mean(fake_output)\n",
    "\n",
    "            # Train the discriminator for each step with gradient descent\n",
    "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "            \n",
    "            # Clip the weights of the discriminator's layers\n",
    "            t=0\n",
    "            for t in range(number_of_disc_layers):\n",
    "                y = tf.clip_by_value(discriminator.trainable_weights[t],\n",
    "                                     clip_value_min=-0.05,\n",
    "                                     clip_value_max=0.05,\n",
    "                                     name=None)\n",
    "                discriminator.trainable_weights[t].assign(y)\n",
    "            \n",
    "            # Train the generator only for the last step\n",
    "            if i == (n_steps-1) :\n",
    "                \n",
    "                # Create fake image and predict if they are true thank to the discriminator \n",
    "                fake_training_data = generator(x)\n",
    "                fake_output = discriminator(fake_training_data)\n",
    "                \n",
    "                # The loss for the generator is the inverse of the mean of fake output\n",
    "                # The generator goal is to fool the discriminator\n",
    "                gen_loss = -tf.reduce_mean(fake_output)\n",
    "                \n",
    "                # Gradient descent\n",
    "                gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "                generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(epoch, train_x, nex=8, zm=2):\n",
    "    \n",
    "    samples = generator(tf.random.normal([train_x.shape[0], 1, 1, 64]))\n",
    "    fig, axs = plt.subplots(ncols=nex, nrows=1, figsize=(zm * nex, zm))\n",
    "    \n",
    "    for axi in range(nex):\n",
    "        axs[axi].matshow(samples.numpy()[axi].squeeze(),\n",
    "                         cmap=plt.cm.Greys,\n",
    "                         vmin=0,\n",
    "                         vmax=1)\n",
    "        axs[axi].axis('off')\n",
    "    \n",
    "    plt.savefig(os.path.join(\"model\",\"gan\",'gan_results_epoch_%03d.png' %(epoch)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epoch, train_dataset, generator_optimizer, discriminator_optimizer, losses, N_TRAIN_BATCHES, N_TEST_BATCHES):\n",
    "    start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # train\n",
    "        for batch, train_x in tqdm(zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES):\n",
    "            train_step(train_x, generator_optimizer, discriminator_optimizer)\n",
    "\n",
    "        # test\n",
    "        loss = []\n",
    "        for batch, test_x in tqdm(zip(range(N_TEST_BATCHES), test_dataset), total=N_TEST_BATCHES):\n",
    "            loss.append(compute_loss(train_x))\n",
    "        losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "\n",
    "        # plot results\n",
    "        display.clear_output()\n",
    "        print(\"Epoch: {} | disc_loss: {} | gen_loss: {}\".format(epoch, \n",
    "                                                                losses.disc_loss.values[-1], \n",
    "                                                                losses.gen_loss.values[-1]))\n",
    "        plot_reconstruction(epoch, test_x)\n",
    "\n",
    "    time_to_train_gan = time.time() - start\n",
    "    tf.print ('Time for the training is {} sec,'.format( time.time() - start))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizers(gen_learning_rate, disc_learning_rate):\n",
    "    \n",
    "    # RMSprop optimizer\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(gen_learning_rate)\n",
    "    discriminator_optimizer = tf.keras.optimizers.RMSprop(disc_learning_rate)\n",
    "    \n",
    "    return generator_optimizer, discriminator_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator()\n",
    "discriminator = get_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF=60000\n",
    "BATCH_SIZE=512\n",
    "TEST_BUF=10000\n",
    "DIMS = (28,28,1)\n",
    "N_TRAIN_BATCHES =int(TRAIN_BUF/BATCH_SIZE)\n",
    "N_TEST_BATCHES = int(TEST_BUF/BATCH_SIZE)\n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "generator_optimizer, discriminator_optimizer = get_optimizers(gen_learning_rate=0.00003, disc_learning_rate = 0.00005)\n",
    "\n",
    "losses = pd.DataFrame(columns = ['disc_loss', 'gen_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(TRAIN_BUF, BATCH_SIZE, TEST_BUF, data_type=\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train(n_epochs, train_dataset, generator_optimizer, discriminator_optimizer, \n",
    "               losses, N_TRAIN_BATCHES=N_TRAIN_BATCHES, N_TEST_BATCHES=N_TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = datetime.now().timestamp()\n",
    "generator_path = os.path.join(\"model\", 'attack_gan_model_%04d.h5' % (t1))\n",
    "generator.save(generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_path = os.path.join(\"model\", 'discriminator_model_%04d.h5' % (t1))\n",
    "discriminator.save(discriminator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.legend([\"discriminator\", \"generator\"])\n",
    "plt.savefig(os.path.join(\"model\", 'gan_loss_%04d.png') % (t1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_load = load_model(generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_load = load_model(discriminator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_load.predict(tf.random.normal([1, 1, 1, 64])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_load.predict(gen_load.predict(tf.random.normal([1, 1, 1, 64])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
