{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy - attack model - part1 : Wasserstein GAN\n",
    "\n",
    "reference for the WGAN : https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.sys.path.append(\"./src\")\n",
    "from utils import plot_img\n",
    "from utils import load_mnist_data\n",
    "from utils import pick_and_show_image\n",
    "from models import define_critic\n",
    "\n",
    "from models import define_critic\n",
    "from models import generate_latent_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train, x_test, y_test = load_mnist_data(\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 load the model attacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "target_model = load_model(\"model/target_model_1583961212.741507.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 load the critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the id of the model. For example we choose the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if re.search(r'^(attack_critic)*', 'attack_critic_model_weights_123.h5'):\n",
    "#     print(re.search(r'^attack_critic_\\w+_(\\d*)\\.h5$', 'attack_critic_model_weights_123.h5').groups()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"./model\"\n",
    "# files = [file.split(\"_\")[-1].split(\".\")[0] for file in os.listdir(model_dir)\n",
    "#          if re.search(r'^attack_critic_\\w+_(\\d*)\\.h5$', file)]\n",
    "# model_num = [int(file) for file in files if not(file in ['checkpoints','loss','model'])]\n",
    "# model_id = np.max(model_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the critic model is customed we build up a new model with the same architecture as the critic, load the weights of the one saved and load them into the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_critic_model = define_critic()\n",
    "# attack_critic_model.load_weights(\"model/attack_critic_model_weights_\" + str(model_id) + \".h5\")\n",
    "# attack_critic_model._name = \"attack_critic_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_critic_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 13770\n",
    "attack_critic_model = define_critic()\n",
    "attack_critic_model.load_weights(\"model/attack_critic_model_weights_\" + str(model_id) + \".h5\")\n",
    "attack_critic_model._name = \"attack_critic_model_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_critic_model = load_model(\"model/attack_critic_model_weights_13770.h5\")\n",
    "# attack_critic_model._name = \"attack_critic_model\"\n",
    "# attack_critic_model.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 load the gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two gan with different names so that the model do not have the same layers' name when combined in the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_gan_model_1 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "# attack_gan_model_1._name = \"attack_gan_model_1\"\n",
    "# attack_gan_model_2 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "# attack_gan_model_2._name = \"attack_gan_model_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_gan_model_1 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "attack_gan_model_1._name = \"attack_gan_model_1\"\n",
    "print(attack_gan_model_1.name)\n",
    "\n",
    "attack_gan_model_2 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "attack_gan_model_2._name = \"attack_gan_model_2\"\n",
    "print(attack_gan_model_2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design a model which intermediate layer is of the dimension of the latent space. We want to generate a point in the latent space (__x__) that makes the generator produce a credible image (hand written-digit-like) which will be classified by the target model as the target label.\n",
    "The outputs of the model are chosen to build a loss designed to perform this optimization program :\n",
    "- __attack_critic_model(attack_gan_model_1(x))__ which the critic of the image generated by the GAN, the more the image is credible, the higher will be this output ;\n",
    "- __target_model(attack_gan_model_2(x))__ which the class predicted by the target model, this one will be used to compute a difference with the label targetted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_model_optim(random_init_dim=1, \n",
    "                           attack_critic_model=attack_critic_model, \n",
    "                           attack_gan_model_1=attack_gan_model_1, \n",
    "                           attack_gan_model_2=attack_gan_model_2):\n",
    "    \n",
    "    # generate a point in the latent space\n",
    "    main_input = Input(shape = (random_init_dim, ), name='main_input')\n",
    "    x = Dense(50,\n",
    "              activation='linear',\n",
    "              kernel_initializer = RandomNormal(mean=0.0, stddev=1, seed=None),\n",
    "              name = \"gen_attack_img\")(main_input)\n",
    "    gen_latent_values = Model(main_input, x)\n",
    "\n",
    "    # build up the model with the outputs that will be used to compute the loss\n",
    "    model = Model(inputs=main_input, outputs = [attack_critic_model(attack_gan_model_1(x)),\n",
    "                                                target_model(attack_gan_model_2(x))])\n",
    "    print(model.summary())\n",
    "    return model, gen_latent_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one layer should be trainable to perform the optimization process. If not we will retrain our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_not_optim_layers(model):\n",
    "    for layer in model.layers:\n",
    "        if layer.name == \"gen_attack_img\":\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable=False\n",
    "        # print(\"layer : {}, trainable {}\".format(layer.name, layer.trainable))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is compound of two parts :\n",
    "- the credibility : is the image reconstructed a credible hand written digit ?\n",
    "- the class proximity : is the class of the reconstructed image, the one targetted ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optim_fn(target, lambda_):\n",
    "    def loss_optim(y_true, y_pred):\n",
    "        # credibility part : this part caracterize the credibility of the reconstructed image in the space of the public data\n",
    "        credibility = backend.mean(y_pred[0], axis=0)\n",
    "        \n",
    "        # proximity part : define the absolute difference between the predicted for the reconstructed image and the class targetted\n",
    "        proximity_to_target = backend.mean(y_pred[1], axis=0) - target\n",
    "        proximity_to_target = backend.abs(proximity_to_target)\n",
    "        proximity_to_target = backend.mean(proximity_to_target)/2\n",
    "        return backend.sum([credibility, lambda_*backend.log(proximity_to_target)], axis=-1)\n",
    "    return loss_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Launch the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim parameters\n",
    "lambda_ = 100\n",
    "n_epochs = 150\n",
    "\n",
    "# inputs are constants and ouputs are not used\n",
    "x = np.full((500, 1), 1)\n",
    "model, _ = create_new_model_optim()\n",
    "y = model.predict(np.full((500, 1), 1)) # we do not use these values, there are just here to avoid errors when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, label = pick_and_show_image(x_train, y_train)\n",
    "# target_probs = target_model.predict(np.reshape(img,(1,28,28,1)))\n",
    "# print(\"target probs : {}\".format(target_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs = np.array([0, 0, 1, 0, 0])\n",
    "target_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Make the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization is repeated 5 times. The trial with best results is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_optim_fn(target_probs, lambda_)\n",
    "optimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # adam à tester\n",
    "batch_size = 32\n",
    "\n",
    "latent_reconstructed_list = []\n",
    "loss_list = []\n",
    "loss_1_list = []\n",
    "loss_2_list = []\n",
    "    \n",
    "for i in range(5) :\n",
    "    optim_model, gen_latent_values = create_new_model_optim()\n",
    "    optim_model = freeze_not_optim_layers(model)\n",
    "    optim_model.compile(optimizer=optimizer,loss=loss)\n",
    "    optim_model.fit(x, y, epochs=n_epochs, batch_size = batch_size)\n",
    "\n",
    "    latent_reconstructed = gen_latent_values.predict(np.full((1, 1), 1))\n",
    "    image_reconstructed = attack_gan_model_2.predict(latent_reconstructed)\n",
    "    credibility = attack_critic_model.predict(image_reconstructed)\n",
    "    gen_probs = target_model.predict(image_reconstructed)\n",
    "\n",
    "\n",
    "    latent_reconstructed_list.append(latent_reconstructed)\n",
    "\n",
    "    y_pred = optim_model.predict(x)\n",
    "    loss_result_1 = np.mean(y_pred[1], axis=0) - target_probs\n",
    "    loss_result_1 = np.abs(loss_result_1)\n",
    "    loss_result_1 = np.mean(loss_result_1)/2\n",
    "    loss_result_1 = lambda_*loss_result_1\n",
    "    loss_1_list.append(loss_result_1)\n",
    "        \n",
    "    loss_result_2 = np.mean(y_pred[0], axis=0)\n",
    "    loss_2_list.append(loss_result_2)\n",
    "        \n",
    "    loss_result = loss_result_1 + loss_result_2\n",
    "    loss_list.append(loss_result)\n",
    "\n",
    "    print(\"credibility : {}\".format(credibility))\n",
    "    print(\"classification : {}\".format(gen_probs))\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best one and save the experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reconstructed = attack_gan_model_2.predict(latent_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_reconstructed[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"results\" in os.listdir(\"./\"):\n",
    "    os.mkdir(\"results\")\n",
    "file_name = \"result_\" + str(len(os.listdir(\"./results\")))\n",
    "dir_name = \"./results\"\n",
    "file_path = os.path.join(dir_name, file_name)\n",
    "np.save(file_path, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
