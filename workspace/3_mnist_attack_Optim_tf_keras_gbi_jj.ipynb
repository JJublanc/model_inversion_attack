{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy - attack model - part2 : Optimisation\n",
    "\n",
    "reference for the WGAN : https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "os.sys.path.append(\"./src\")\n",
    "from utils import plot_img\n",
    "from utils import load_mnist_data\n",
    "from utils import pick_and_show_image\n",
    "from models import define_critic\n",
    "\n",
    "from models import define_critic\n",
    "from models import generate_latent_points\n",
    "\n",
    "from models import ClipConstraint\n",
    "\n",
    "import codecs, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 load critic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the critic model is customed we build up a new model with the same architecture as the critic, load the weights of the one saved and load them into the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_critic_model(model_id):\n",
    "    if 'attack_critic_model' in locals():\n",
    "        del attack_critic_model\n",
    "    attack_critic_model = define_critic()\n",
    "    attack_critic_model.load_weights(\"model/attack_critic_model_weights_\" + str(model_id) + \".h5\")\n",
    "    attack_critic_model._name = \"attack_critic_model\"\n",
    "    print(\"Loading of the critic : {}\".format(attack_critic_model.name))\n",
    "    \n",
    "    return attack_critic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 load the gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two gan with different names so that the model do not have the same layers' name when combined in the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan_model(model_id):\n",
    "    \n",
    "    if 'attack_gan_model_1' in locals():\n",
    "        del attack_gan_model_1\n",
    "    attack_gan_model_1 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "    attack_gan_model_1._name = \"attack_gan_model_1\"\n",
    "    print(attack_gan_model_1.name)\n",
    "\n",
    "    if 'attack_gan_model_2' in locals():\n",
    "        del attack_gan_model_2\n",
    "    attack_gan_model_2 = load_model(\"model/attack_gan_model_\" + str(model_id) + \".h5\")\n",
    "    attack_gan_model_2._name = \"attack_gan_model_2\"\n",
    "    print(attack_gan_model_2.name)\n",
    "    \n",
    "    return attack_gan_model_1, attack_gan_model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define optimization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design a model which intermediate layer is of the dimension of the latent space. We want to generate a point in the latent space (__x__) that makes the generator produce a credible image (hand written-digit-like) which will be classified by the target model as the target label.\n",
    "The outputs of the model are chosen to build a loss designed to perform this optimization program :\n",
    "- __attack_critic_model(attack_gan_model_1(x))__ which the critic of the image generated by the GAN, the more the image is credible, the higher will be this output ;\n",
    "- __target_model(attack_gan_model_2(x))__ which the class predicted by the target model, this one will be used to compute a difference with the label targetted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_model_optim(target_model,\n",
    "                           attack_critic_model, \n",
    "                           attack_gan_model_1, \n",
    "                           attack_gan_model_2,\n",
    "                           random_init_dim=1):\n",
    "    \n",
    "    if 'gen_latent_values' in locals():\n",
    "        del gen_latent_values\n",
    "    \n",
    "    if 'gen_attack_img' in locals():\n",
    "        del gen_attack_img\n",
    "    \n",
    "    # generate a point in the latent space\n",
    "    main_input = Input(shape = (random_init_dim, ), name='main_input')\n",
    "    x = Dense(50,\n",
    "              activation='linear',\n",
    "#              kernel_initializer = RandomNormal(mean=0.0, stddev=1, seed=None),\n",
    "              kernel_initializer = RandomUniform(-2, 2, seed=None),\n",
    "              kernel_constraint = ClipConstraint(2),\n",
    "              use_bias=False ,\n",
    "              name = \"gen_attack_img\")(main_input)\n",
    "    gen_latent_values = Model(main_input, x)\n",
    "\n",
    "    # build up the model with the outputs that will be used to compute the loss\n",
    "    model = Model(inputs=main_input, outputs = [attack_critic_model(attack_gan_model_1(x)),\n",
    "                                                target_model(attack_gan_model_2(x))])\n",
    "    return model, gen_latent_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one layer should be trainable to perform the optimization process. If not we will retrain our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_not_optim_layers(model):\n",
    "    for layer in model.layers:\n",
    "        if layer.name == \"gen_attack_img\":\n",
    "            layer.trainable=True\n",
    "        else:\n",
    "            layer.trainable=False\n",
    "        # print(\"layer : {}, trainable {}\".format(layer.name, layer.trainable))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is compound of two parts :\n",
    "- the credibility : is the image reconstructed a credible hand written digit ?\n",
    "- the class proximity : is the class of the reconstructed image, the one targetted ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optim_fn(target, lambda_):\n",
    "    def loss_optim(y_true, y_pred):\n",
    "        # target.shape -> (5,)\n",
    "        # y_pred is a list with\n",
    "        # y_pred[0].shape -> (500, 1)\n",
    "        # y_pred[1].shape -> (500, 5)\n",
    "        # credibility part : this part caracterize the credibility of the reconstructed image in the space of the public data\n",
    "        credibility = backend.mean(y_pred[0], axis=0)\n",
    "        # credibility.shape -> (1,)\n",
    "        \n",
    "        # proximity part : define the absolute difference between the predicted for the reconstructed image and the class targetted\n",
    "        # backend.mean(y_pred[1], axis=0).shape -> (5,)\n",
    "        proximity_to_target = backend.mean(y_pred[1], axis=0) - target\n",
    "        proximity_to_target = backend.abs(proximity_to_target)\n",
    "        proximity_to_target = backend.mean(proximity_to_target)/2\n",
    "        return backend.sum([credibility, lambda_*backend.log(proximity_to_target)], axis=-1)\n",
    "    return loss_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Launch the whole process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the optimization loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization is repeated 5 times. The trial with best results is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_and_fit_optim_model(x, y, target_model, n_epochs, model_id, loss, optimizer, batch_size):\n",
    "    attack_critic_model_loop = load_critic_model(model_id)\n",
    "    attack_gan_model_a, attack_gan_model_b = load_gan_model(model_id)\n",
    "        \n",
    "    # define optimization models\n",
    "    optim_model, gen_latent_values = create_new_model_optim(target_model,\n",
    "                                                                attack_critic_model_loop, \n",
    "                                                                attack_gan_model_a, \n",
    "                                                                attack_gan_model_b,)\n",
    "    optim_model = freeze_not_optim_layers(optim_model)\n",
    "    optim_model.compile(optimizer=optimizer,loss=loss)\n",
    "    optim_model.fit(x, y, epochs=n_epochs, batch_size = batch_size)\n",
    "    \n",
    "    latent_reconstructed = gen_latent_values.predict(np.full((1, 1), 1))\n",
    "    image_reconstructed = attack_gan_model_a.predict(latent_reconstructed)\n",
    "    probs_reconstructed = target_model.predict(image_reconstructed)\n",
    "    \n",
    "    return optim_model, gen_latent_values, probs_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(target_probs, x, optim_model, _lambda):\n",
    "    y_pred = optim_model.predict(x)\n",
    "            # loss part 1\n",
    "    loss_result_1 = np.mean(y_pred[1], axis=0) - target_probs\n",
    "    loss_result_1 = np.abs(loss_result_1)\n",
    "    loss_result_1 = np.mean(loss_result_1)/2\n",
    "    loss_result_1 = lambda_*loss_result_1\n",
    "            # loss part 2\n",
    "    loss_result_2 = np.mean(y_pred[0], axis=0)\n",
    "            # loss\n",
    "    loss_result = loss_result_1 + loss_result_2\n",
    "    \n",
    "    return loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_loop_repeated(target_probs, n_epochs, model_id, target_model_name, loss, optimizer, n=5, batch_size=32):\n",
    "\n",
    "    target_model = load_model(target_model_name)\n",
    "    \n",
    "    x = np.full((500, 1), 1)\n",
    "    y = [np.full((500, 1), 1), np.full((500, 5), 1)]\n",
    "    \n",
    "    image_reconstructed_list = []\n",
    "    latent_reconstructed_list = []\n",
    "    probs_reconstructed_list = []\n",
    "    loss_list = []\n",
    "    loss_1_list = []\n",
    "    loss_2_list = []\n",
    "    \n",
    "    test = []\n",
    "\n",
    "    for i in range(n) :\n",
    "        \n",
    "        optim_model, gen_latent_values, probs_reconstructed = init_and_fit_optim_model(x, y, target_model, n_epochs, model_id, loss, optimizer, batch_size)\n",
    "        \n",
    "        if np.argmax(probs_reconstructed)==np.argmax(target_probs):\n",
    "            optim_model.fit(x, y, epochs=3*n_epochs, batch_size = batch_size)\n",
    "            \n",
    "            latent_reconstructed = gen_latent_values.predict(np.full((1, 1), 1))\n",
    "            image_reconstructed = attack_gan_model_a.predict(latent_reconstructed)\n",
    "            probs_reconstructed = target_model.predict(image_reconstructed)\n",
    "            loss = compute_loss(target_probs, x, optim_model, _lambda)\n",
    "            \n",
    "            latent_reconstructed_list.append(latent_reconstructed)\n",
    "            image_reconstructed_list.append(image_reconstructed)\n",
    "            probs_reconstructed_list.append(probs_reconstructed)\n",
    "            loss_list.append(loss)\n",
    "    \n",
    "    if len(loss_list)==0:\n",
    "        return None, None, None, None, None, None\n",
    "    else:\n",
    "        return latent_reconstructed_list, loss_list, image_reconstructed_list, probs_reconstructed_list, optim_model, gen_latent_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_loop_repeated(target_probs, 1, model_id, target_model_name, loss, optimizer, n=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define saving process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_results_df(): \n",
    "    if \"results\" not in os.listdir():\n",
    "        os.mkdir(\"results\")\n",
    "    \n",
    "    if \"images\" not in os.listdir(\"results\"):\n",
    "        os.mkdir(\"results/images\")\n",
    "    \n",
    "    if \"latent_vects\" not in os.listdir(\"results\"):\n",
    "        os.mkdir(\"results/latent_vects\")\n",
    "\n",
    "    if \"results.csv\" not in os.listdir(\"results\"):\n",
    "        dict_results = ['model_id',\n",
    "                             'n_epochs',\n",
    "                             'target_model_name',\n",
    "                             'latent_reconstructed',\n",
    "                             'true_value',\n",
    "                             'prediction',\n",
    "                             'date',\n",
    "                             'loss',\n",
    "                             'image_reconstructed']\n",
    "\n",
    "        pd_results = pd.DataFrame(columns = dict_results)\n",
    "    else :\n",
    "        pd_results = pd.read_csv(\"results/results.csv\")\n",
    "                        \n",
    "    return pd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_of_results(target_probs, target_model_name, loss_list, \n",
    "                         latent_reconstructed_list, probs_reconstructed_list, \n",
    "                         image_reconstructed_list, num):\n",
    "        target = np.argmax(target_probs)\n",
    "        date = datetime.datetime.now().timestamp()\n",
    "        evaluation_model = load_model(evaluation_model_name)\n",
    "\n",
    "        loss = np.min(loss_list)\n",
    "        best_position =  np.argmin(loss_list)\n",
    "        best_latent_reconstructed = latent_reconstructed_list[best_position]\n",
    "        # best_target_prediction = np.argmax(probs_reconstructed_list[best_position])\n",
    "        best_image_reconstructed = image_reconstructed_list[best_position]\n",
    "        best_target_prediction = np.argmax(evaluation_model.predict(best_image_reconstructed))\n",
    "        \n",
    "        best_latent_reconstructed_name = \"latent_{}_{}\".format(target, num)\n",
    "        best_image_reconstructed_name = \"image_{}_{}\".format(target, num)\n",
    "\n",
    "        # create the dict of the results\n",
    "        result_dict = {'model_id': model_id,\n",
    "                       'n_epochs': n_epochs,\n",
    "                       'n_seeds': n_seeds,\n",
    "                       'target_model_name': target_model_name,\n",
    "                       'latent_reconstructed': best_latent_reconstructed_name,\n",
    "                       'true_value': target,\n",
    "                       'prediction_evaluation': best_target_prediction,\n",
    "                       'date': date,\n",
    "                       'loss': np.min(loss_list),\n",
    "                       'image_reconstructed': best_image_reconstructed_name}\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_mnist_data(\"private\")\n",
    "img, label = pick_and_show_image(x_train, y_train)\n",
    "img_rec = np.expand_dims(np.expand_dims(img,0),-1)\n",
    "make_dict_of_results(target_probs, target_model_name, loss_list, [np.full((500, 1), 1)], [0,1,0,0,0,0], [img_rec], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_experiment_and_save_results(target_probs,\n",
    "                                       n_epochs,\n",
    "                                       model_id,\n",
    "                                       target_model_name,\n",
    "                                       evaluation_model_name,\n",
    "                                       loss,\n",
    "                                       optimizer,\n",
    "                                       n_seeds,\n",
    "                                       batch_size=32):\n",
    "    \n",
    "    # launch the experiment\n",
    "    latent_reconstructed_list, loss_list,\\\n",
    "    image_reconstructed_list, probs_reconstructed_list, optim_model, gen_latent_values = optim_loop_repeated(target_probs, \n",
    "                                                                             n_epochs,\n",
    "                                                                             model_id,\n",
    "                                                                             target_model_name,\n",
    "                                                                             loss,\n",
    "                                                                             optimizer,\n",
    "                                                                             n_seeds,\n",
    "                                                                             batch_size=32)\n",
    "    if loss_list!=None:\n",
    "        # create the dataframe of the results\n",
    "        pd_results = return_results_df()\n",
    "        num = len(pd_results)\n",
    "\n",
    "        # build up objects to save\n",
    "        result_dict = make_dict_of_results(target_probs,\n",
    "                                           evaluation_model_name,\n",
    "                                           loss_list,\n",
    "                                           latent_reconstructed_list,\n",
    "                                           probs_reconstructed_list,\n",
    "                                           image_reconstructed_list,\n",
    "                                           num)\n",
    "\n",
    "        # save the csv file\n",
    "        pd_results = pd_results.append(pd.Series(result_dict), ignore_index=True)\n",
    "        pd_results.to_csv(\"results/results.csv\", index=False)\n",
    "\n",
    "        # save the latent vect\n",
    "        np.save(\"results/latent_vects/{}\".format(results_dict[\"latent_reconstructed\"]), best_latent_reconstructed)\n",
    "\n",
    "        # save the image\n",
    "        np.save(\"results/images/{}\".format(results_dict[\"image_reconstructed\"]), best_image_reconstructed)\n",
    "\n",
    "        return loss_list, probs_reconstructed_list, image_reconstructed_list, latent_reconstructed_list, optim_model, gen_latent_values\n",
    "    else:\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Settle parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models used\n",
    "target_model_name = \"model/target_model.h5\"\n",
    "evaluation_model_name = \"model/evaluation_model.h5\"\n",
    "model_id = 13770\n",
    "\n",
    "# optim parameters\n",
    "lambda_ = 3000\n",
    "n_epochs = 1\n",
    "n_seeds = 2\n",
    "\n",
    "# target\n",
    "target_probs = np.array([0, 0, 1, 0, 0])\n",
    "\n",
    "# Optim parameters\n",
    "# TODO : mettre en paramètre le caractéristiques du programme d'optimisation\n",
    "loss = loss_optim_fn(target_probs, lambda_)\n",
    "optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) # adam à tester\n",
    "batch_size = 32\n",
    "\n",
    "# target\n",
    "target_probs = [0, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Launch the optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list, probs_reconstructed_list, image_reconstructed_list, \\\n",
    "latent_reconstructed_list, optim_model, gen_latent_values = launch_experiment_and_save_results(target_probs,\n",
    "                                                                                               n_epochs,\n",
    "                                                                                               model_id,\n",
    "                                                                                               target_model_name,\n",
    "                                                                                               evaluation_model_name,\n",
    "                                                                                               loss,\n",
    "                                                                                               optimizer,\n",
    "                                                                                               n_seeds,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test[0][0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.argmax(x) for x in probs_reconstructed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_reconstructed_list[6][0,:,:,0])\n",
    "plt.savefig(\"un.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test[1][0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = attack_gan_model_1.predict(latent_vect/np.max(latent_vect))\n",
    "plt.imshow(img[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_title(result_num):\n",
    "    pd_results = pd.read_csv(\"results/results.csv\")\n",
    "    truth = pd_results.loc[result_num].true_value\n",
    "    pred = pd_results.loc[result_num].prediction\n",
    "    title = \"Value to find : {} / value found : {}\".format(truth, pred)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(num):\n",
    "    image_np = np.load(\"results/images/image_{}.npy\".format(num))[0,:,:,0]\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(write_title(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(result_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_results = pd.read_csv(\"results/results.csv\")\n",
    "pd_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
